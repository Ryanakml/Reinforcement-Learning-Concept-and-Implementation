{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ec4ff-06b4-4a46-947a-2b3e87929707",
   "metadata": {},
   "source": [
    "# Math Behind Reinforcement Learning (Not for Dummies)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44859821-409c-4989-8eb6-f99b2a41df30",
   "metadata": {},
   "source": [
    "**RL : Agent learns to make decision by interaction with an environment, goal of agent is maximize reward overtime.**\n",
    "\n",
    "## Basic Component :\n",
    "\n",
    "1. Agent, Learner/decision maker.\n",
    "2. Environment, Where agent operates.\n",
    "3. States(s), current situation of environment.\n",
    "4. Action(a), what can agent do in that state.\n",
    "5. Reward(r), feedback after that action(+ or -).\n",
    "6. Policy(π), strategy that agent follow to choose action.\n",
    "7. Value Action, Predict how good a state, in term of future r.\n",
    "\n",
    "---\n",
    "\n",
    "## Markov Decision Process (MDP)\n",
    "\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Bellman-Equation.png?ssl=1)\n",
    "\n",
    "MDP models the environment with this element:\n",
    "\n",
    "1. State (S)\n",
    "2. Action (A)\n",
    "3. Transition Probabilities (P) = P(s'|s,a) : The probability of moving to state s' after taking action a in state s.\n",
    "4. Reward Funtion (R) = R(s,a) : Expected reward for each state and action\n",
    "5. Discount Factor (γ) : how much future rewards are worth to immediate ones (0 < γ < 1).\n",
    "\n",
    "**The Markov Property** : The next state and reward depend only on the current state and action, not on full history.\n",
    "\n",
    "**Goal of RL** : \n",
    "\n",
    "*\"Find the expected value ( policy π(a|s) ) that maximizes the expected cumulative reward over time, also called the return\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a8c7a-71f5-46c1-a865-d18550e07e40",
   "metadata": {},
   "source": [
    "## Transition Probability\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VmV-tIr2e1eX24Y_0KMi5w.png)\n",
    "\n",
    "\n",
    "**Transition Probability tells that :**\n",
    "\n",
    "*\"if you are in certain situation (state i) and take a certain action. whats the chance you will end up in another situation (next state i+1...)\"*\n",
    "\n",
    "or \n",
    "\n",
    "*\"If im here and do this, how likely it is making me end up there\"*\n",
    "\n",
    "**for example.**\n",
    "\n",
    "- you are in a terminal batoh(si) \n",
    "- taking bus (ai)\n",
    "- youre likely 90% probability will go to terminal darussalam(si+1)\n",
    "- and 30% in terminal ulelue(si+2).\n",
    "- That 90% probability is transition probabilty p(si+1 | si,ai). \n",
    "\n",
    "\n",
    "**But why?..**\n",
    "\n",
    "Let’s say your RL agent is a robot in a room:\n",
    "\n",
    "- State s: robot is in the center\n",
    "- Action a: move north\n",
    "- New state s’:\n",
    "- 80% chance: ends up one step north\n",
    "- 20% chance: slips and stays in place\n",
    "\n",
    "*\"The agent needs to learn that “move north” doesn’t always succeed — this randomness is captured in p(s’|s,a).\"*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b15c63-2b02-4643-9b7e-6efd568e864f",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Return\n",
    "\n",
    "**Random Variable :**\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FdQcldGubZNbfJRrh1GO8g.png)\n",
    "\n",
    "\n",
    "\n",
    "**Return (Random Variable with Discounted Rate) :**\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZEtDC9eBwSVsQ8_jtnuudg.png)\n",
    "\n",
    "**Return tells that :**\n",
    "\n",
    "*\"the total reward agent gets from time t onward\"*\n",
    "\n",
    "- γ (gamma) ∈ [0, 1]\n",
    "- Closer to 1 = future matters more\n",
    "- Closer to 0 = only cares about immediate reward\n",
    "\n",
    "\n",
    "**Why?**\n",
    "\n",
    "*agent goal is maximize Gt, since it tells total reward agent has, and we want to maximize this reward*\n",
    "\n",
    "**For example..**\n",
    "- each coin = reward\n",
    "- Gt = total coins from now until game ends\n",
    "- Choise :\n",
    "- stay and get 1 coin\n",
    "- risk more for now and get 10 coin later\n",
    "- Gt is there for decide that choise.\n",
    "\n",
    "\n",
    "*\"We model terms like return and value as random variables because the environment is unpredictable — the same action doesn’t always give the same result.\"*\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXQRwn5toJQXcJsBZ8HqMg.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4de413-3b21-477f-a572-1126bcd5408e",
   "metadata": {},
   "source": [
    "so, when we goes from state s to next state s' by taking action a. the reward r it gets is not always the same.\n",
    "\n",
    "*\"Reward comes from an unknown distribution - random\"*\n",
    "\n",
    "instead if relying on one time reward, we compute :\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*6bnA-TulIcQDiRH_7_sBRg.png)\n",
    "\n",
    "or\n",
    "\n",
    "the expected reward : E[r | s, a, s’] \n",
    "\n",
    "*\"the average of all possible rewards for that transition.\"*\n",
    "\n",
    "for example ..\n",
    "\n",
    "- at casino\n",
    "- pull a lever (action a) many time\n",
    "- each time, even the a is same, reward might be different. (5 coin, 10 coin, and even 0)\n",
    "\n",
    "*'we didnt know exact reward we get next, but overtime, we can estimate'*\n",
    "\n",
    "- on average, the lever (action a) gives me 6 coin, so thats 6 coin is our expected reward.\n",
    "\n",
    "*'reward now and next reward in same action over time is being averaged'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71c2f6-1f46-4aee-afc0-bbc4982bf2bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## State Value Function \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IYC5S0lG7ov-surZ9wwMfg.png)\n",
    "\n",
    "**State Value Function tells that :**\n",
    "\n",
    "*\"How good is for the agent to be in a state s, assuming it follow policy π or rule.\"*\n",
    "\n",
    "or \n",
    "\n",
    "*\"Start from state s, than Follow policy π, Then compute the expected return (Gₜ) — the sum of rewards you’ll likely get = How Good that State Is..\"*\n",
    "\n",
    "**for example ..**\n",
    "\n",
    "**State (S):**\n",
    "- S1 = Kitchen\n",
    "- S2 = Living Room\n",
    "- S3 = Charging Station\n",
    "\n",
    "**Action (A):**\n",
    "- A1 = Walk\n",
    "- A2 = Clean\n",
    "- A3 = Charge\n",
    "\n",
    "**Reward (R):**\n",
    "- Cleaning = +10\n",
    "- Walking = -1\n",
    "- Charging = +5\n",
    "\n",
    "\n",
    "**T = 0 ...**\n",
    "\n",
    "\n",
    "**State :**\n",
    "\n",
    "- S2 (Living Room)\n",
    "\n",
    "**Policy π :**\n",
    "- 50% chance choose clean (A2)\n",
    "- 50% change choose walk (A1) to kitchen (S1)\n",
    "\n",
    "\n",
    "**Randomness :**\n",
    "- if choose walk (A1), there is a 20% chance the robot slips and ends up Charging Station (S3)\n",
    "\n",
    "so :\n",
    "\n",
    "- transition (p(s'|s,a)) is not guaranteed when played.\n",
    "- reward will differ depending on where it lands.\n",
    "\n",
    "\n",
    "**The Question is :**\n",
    "\n",
    "*\"How good is the Living Room (S2)\"*\n",
    "\n",
    "which is :\n",
    "\n",
    "State Value Function (Vπ(S2))\n",
    "\n",
    "\n",
    "**How we determine how good a state is? :**\n",
    "\n",
    "1. From s (s2 : Kitchen), robot follows policy π.\n",
    "\n",
    "2. Transition Probability :\n",
    "-  50% clean (A2) = (+10 reward)\n",
    "-  50% walk (A1) = (random outcome) :\n",
    "  \n",
    "    -  80% kitchen (S1) (then can Clean (A2) next = +10 future reward)\n",
    "    -  20% Charging Station (S3) (Charge (A3) = +5 reward)\n",
    "   \n",
    "3. Each path give different total reward (Gt).\n",
    "\n",
    "Since Transition & Reward is uncertain or random,  we :\n",
    "\n",
    "*\"Take the average of all these possible outcomes or reward (Gt)\"*\n",
    "\n",
    "or \n",
    "\n",
    "*\"How good the state (S) Living Room (S2) is\"*\n",
    "\n",
    "which is :\n",
    "\n",
    "State Value Function : Vπ(S2) = E[Gₜ | s = S2]\n",
    "\n",
    "or\n",
    "\n",
    "*\"Expected total reward from State s (S2) follow policy π\"*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf4ed4-ea3c-4438-81bb-348cd89429bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
